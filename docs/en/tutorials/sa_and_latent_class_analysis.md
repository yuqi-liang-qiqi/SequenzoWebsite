# Understanding Differences between Sequence Analysis, Latent Class Analysis, and Hidden Markov Models

When studying individuals’ life courses, e.g., how people move from school to work to family life, we often ask: **Are there typical patterns?** 

Several methods can be used to answer this classification question. Researchers in life course studies have compared Sequence Analysis (SA) with other approaches that also aim to uncover typical patterns, particularly Latent Class Analysis (LCA) (e.g., Han et al., 2017; Barban & Billari, 2012).

Identifying typical trajectory patterns is undoubtedly one of the most common uses of sequence analysis, particularly when combined with cluster analysis. However, this practice is sometimes treated as the defining feature of the method. In fact, many studies that refer to “sequence analysis” are more precisely performing dissimilarity-based clustering in sequence analysis, which is a term used by Henning and Liao (2018) that we recommend adopting for clarity.

In this tutorial, we focus on clarifying how dissimilarity-based clustering in sequence analysis differs from other methods that similarly reveal typical patterns, especially latent class analysis and hidden markov models (Helske et al., 2018).

Other conceptual tutorials will cover what else sequence analysis can do, such as polyadic sequence analysis, sequence history analysis, and sequence analysis multi-state models.

## 1. Why do we compare them?

Some individuals may follow a “traditional” path (school → job → marriage → children), while others may delay marriage, cohabit, or remain single.

When thinking this way, what has been frequently neglected is that we are already making an implicit assumption: that thousands of individuals in our data can be grouped into a few typical patterns or life course types (often called "typologies" in the sequence analysis community).

To identify these types of trajectories, researchers have developed several approaches:

* **Dissimilarity-based clustering in sequence analysis:** a data-driven, distance-based approach
* **Latent class analysis (LCA):** a model-based, probability-driven approach
* **Hidden markov models (HMMs):** a state-transition approach based on probabilistic rules over time

At first glance, all three methods seem to aim for the same goal as they group/classify individuals with similar life courses. But under the surface, they rely on very different logics:

* Dissimilarity-based clustering in SA looks for similarity in observed patterns, 
* LCA infers hidden population types that generate those patterns, and 
* HMMs model hidden life stages that evolve dynamically based on transitions over time.

Understanding these differences helps clarify what each method can (and cannot) reveal about trajectories.

## 2. The basic intuition

Imagine you have the 10-year health histories of 100 older adults, represented by a sequence of letters:

```
HHHMMLLLDDDD
```

H = healthy, M = mild symptoms, L = limited mobility, D = dependent.

Now, how do you find typical health trajectories among these individuals?

### Dissimilarity-based clustering in sequence analysis: Comparing life stories directly

Dissimilarity-based clustering in sequence analysis treats each person’s life as a string of letters and simply asks:

> “How different is person A’s life from person B’s?”

In order to calculate, we need to use [dissimilarity measures](../function-library/get-distance-matrix.md), one of the most important components of sequence analysis. If we use optimal matching (OM), one of the most commonly used dissimilarity measures, it will calculate how many small changes (insertions, deletions, substitutions) are needed to turn one sequence into another. It is a bit like measuring how many edits it takes to make two words look the same, which is why such a dissimilarty measure falls under the category of "edit distance". 

For instance, if two women's lives differ only by a few edits, they are close/similar. If they need many edits, they are dissimilar (far apart).

After calculating these distances for everyone in the data, we use **cluster analysis** to group people whose lives look alike. Commonly, we use hierarchical clustering, particularly the Ward D2 or Ward D methods. 

This approach of combining sequence analysis and cluster analysis is visual and intuitive. You can literally see the sequences and compare them.

### Latent Class Analysis: Modeling hidden life types

Latent class analysis (LCA) takes a very different perspective.

It assumes that everyone’s observed health history is generated by a few **unseen population groups**, or **latent classes**. 

Absolutely — let’s translate that into a more down-to-earth, intuitive story.

---

Imagine you’re running a **bakery** that sells cupcakes.
You notice that your customers tend to like very different combinations of flavors — but you don’t know how many *types* of customers there are, or what each type likes.

You record what everyone buys over a week:
some people mostly get chocolate, some mix chocolate and vanilla, others buy fruit flavors.
Now you want to find the hidden “taste types.”

---

### Step 1: The guess

At first, you **don’t know** what those taste types are.
So you make a few guesses: maybe there are 3 kinds of customers — “chocolate lovers,” “balanced tasters,” and “fruit lovers.”
You also make an initial guess for what each group *might* prefer, like:

| Type             | Chocolate | Vanilla | Fruit |
| ---------------- | --------- | ------- | ----- |
| Chocolate lovers | 0.8       | 0.15    | 0.05  |
| Balanced tasters | 0.4       | 0.4     | 0.2   |
| Fruit lovers     | 0.1       | 0.2     | 0.7   |

These numbers are just your *starting assumptions* — not based directly on the data yet.

---

### Step 2: Compare guesses to reality

Next, you check: given what each “type” is supposed to like,
how likely is it that each real customer’s purchases came from that type?
For example:

* someone who bought chocolate every time probably fits “chocolate lovers” best;
* someone who mixed fruit and vanilla might fit “balanced tasters” a bit better.

You keep track of those likelihoods for everyone.

---

### Step 3: Refine the “recipes”

Now you use those likelihoods to **update your guesses**:
if some customers you thought were “chocolate lovers” actually bought more vanilla than expected,
you slightly reduce the chocolate probability for that group and increase vanilla a bit.
At the same time, you adjust how many customers you think belong to each type (class proportions).

Then you check again — “given these new recipes, who fits where?” — and refine again.

---

### Step 4: Converge on the best-fitting model

After several rounds, the “taste profiles” (your class-specific distributions) settle down so that:

* each type’s flavor preferences match what its customers usually buy, and
* the mix of types explains the overall pattern of sales as well as possible.

At this point, you’ve discovered the **latent classes** of your bakery:
not because you saw them directly in the data, but because you found the combination of class proportions and within-class preferences that best *reproduces* the messy purchase records you observed.

That’s exactly what **LCA** does statistically:
it keeps adjusting the hidden groups and their internal probability tables until the synthetic data they would generate looks as close as possible to the real data.


---

At first glance, both **cluster analysis** (as used in sequence analysis) and **latent class analysis (LCA)** seem to do the same thing: they both turn messy, individual life histories into a few meaningful types.
However, they rest on very different assumptions about *what* those types represent and *how* they are identified.

Using a weather analogy makes the contrast clear:

### Dissimilarity-based clustering in sequence analysis

Imagine a climatologist collects the daily temperature curves of hundreds of cities.
Without assuming anything about the underlying climate systems, she simply **compares the shapes of these curves**. Cities whose temperature patterns look more similar are grouped together.

This is a purely **appearance-based classification** as it relies on measuring how close or distant two observed sequences are.

There is no assumption that these cities share an underlying “generating mechanism.”
The method only says: *these trajectories look alike*, not *these trajectories come from the same process*.
Hence, it provides an **purely empirical grouping**, based on observed similarity in shape or timing.

### Latent Class Analysis (Model-based)

Now imagine that instead of comparing curves directly, we assume that there are only a few **hidden climate systems**, say, Tropical, Continental, and Polar.

Each system generates temperature patterns according to its own probabilistic rules.
A tropical city is likely warm most of the year; a polar city cold; a continental city variable.

LCA estimates:

1. the parameters of these hidden systems (the within-class probability distributions), and
2. the probability that each city belongs to each system.

In this sense, LCA is not grouping by visual similarity, but by **shared data-generating mechanism**.
It finds the model that best explains how the observed data could have been produced.

In short: 

| Aspect                | Dissimilarity-based Clustering in SA              | Latent Class Analysis (LCA)                                        |
| --------------------- | --------------------------------------------- | ------------------------------------------------------------------ |
| **Underlying logic**  | Similarity of observed trajectories           | Probabilistic data-generating model                                |
| **Assumption**        | No hidden structure assumed                   | Each sequence arises from one of a few latent classes              |
| **Basis of grouping** | Distance between sequences                    | Likelihood under class-specific probabilities                      |
| **Output**            | Mostly hard partitions (each sequence → one cluster) | Soft probabilities (each sequence has membership likelihoods)      |
| **Interpretation**    | “These trajectories look similar.”            | “These trajectories are likely generated by the same hidden rule.” |

In short:
**Cluster analysis groups what looks similar.**
**LCA infers what is generated by the same hidden process.**

Both reveal structure in messy data, but they describe fundamentally different kinds of structure as one is descriptive, and the other one is generative.

--- 

### Clustering is non-parametric; LCA is parametric

* **Cluster analysis** is **non-parametric**:
  It doesn’t assume any specific functional form or probability distribution that generated the data.
  It works directly on the observed sequences and their pairwise similarities.
  You can think of it as *pattern recognition* — the method groups trajectories that *look* similar, without assuming any underlying model.

* **Latent Class Analysis (LCA)** is **parametric**:
  It assumes the data were generated from a **finite mixture model**, i.e. that each observed sequence comes from one of a limited number of latent probability distributions.
  Those distributions are parameterized by probabilities (for each state at each time) and estimated by maximizing a likelihood — just like in regression or other model-based approaches.

In short:

| Aspect             | Sequence Analysis (Clustering)                       | Latent Class Analysis                                                                   |
| ------------------ | ---------------------------------------------------- | --------------------------------------------------------------------------------------- |
| **Type**           | Non-parametric (algorithmic)                         | Parametric (model-based)                                                                |
| **Assumption**     | No assumed distribution; only distance or similarity | Data are generated by a mixture of latent distributions                                 |
| **Estimation**     | No parameters; no likelihood                         | Maximum-likelihood estimation of parameters (class sizes and conditional probabilities) |
| **Interpretation** | Groups that look similar                             | Groups defined by model-based probabilities                                             |
| **Analogy**        | Like k-means or hierarchical clustering              | Like a regression model for categorical longitudinal data                               |

**A simple analogy**

If you think of data analysis as drawing a map:

* **Cluster analysis** groups points on the map based on how close they *appear* to each other.
* **LCA** assumes the map was generated by a few invisible “attractors” (latent classes), each with its own probability cloud and estimates where those attractors are and how strong they are.

> *Cluster analysis is non-parametric; LCA is parametric, in the same sense that regression is.*

Just be careful to add that “parametric” here doesn’t mean continuous parameters or linear models — it means **the model specifies a parametric likelihood function** for how the observed categorical outcomes are generated by latent classes.

### Hidden Markov Models: Capturing dynamic life stages

Hidden Markov Models (HMMs) take yet another perspective. Unlike LCA, which assumes that time points are independent given the latent class, HMMs explicitly model the temporal dependence between states. In other words, HMMs are dynamic models that pay close attention to the timing and sequencing of transitions, much like sequence analysis does.

Specifically, instead of assuming that each person belongs to a fixed group (as in LCA), HMMs assume that **everyone moves through a series of hidden life stages over time** and these stages cannot be observed directly (the reason why the model is called "hidden").

In our health example, what we observe are the outward states, *healthy (H)*, *mild symptoms (M)*, *limited mobility (L)*, *dependent (D)*, recorded each year.

But behind these visible health conditions, there may be underlying *latent life stages/states* (not latent class) such as “robust health,” “emerging decline,” or “chronic frailty.”

These hidden stages/states evolve step by step, following a Markov chain, meaning that each stage depends mainly on the previous one.

At each time point, a hidden stage *emits* an observable state with certain probabilities.
For example, the “emerging decline” stage might usually produce *M* (mild symptoms) but occasionally *H* (healthy) or *L* (limited mobility).

> Note: “Emits” is a technical term from probabilistic modeling. 
> 
> It means that a hidden state produces or generates an observable outcome according to some probability distribution.

By estimating these emission and transition probabilities, HMMs reconstruct both the likely structure of these invisible health stages and how people move among them.

This makes HMMs particularly powerful for studying **transitions and uncertainty**. For example, identifying when people are most likely to shift from robust health to early decline, or how long they typically stay in each hidden phase.

In short, while dissimilarity-based clustering in SA compares whole paths and latent class analysis groups individuals by overall pattern, hidden markov models **reveal the invisible process** that generates the sequence over time.

### How Latent Class Analysis and Hidden Markov Models differ

Both LCA and HMMs deal with things we cannot directly observe, which are *latent structures* hidden behind the data.

But they represent two very different ideas about what those hidden structures mean.

**1. Latent Class Analysis: Static types/classes/groups**

In LCA, the latent variable is a **fixed group membership**. Each person belongs to one unobserved class for the entire period. For example,

* Class 1: mostly healthy, 
* Class 2: gradual decline, or 
* Class 3: early health problems*.

These classes differ in their *overall probabilities* of being in each health state (H, M, L, D) at different ages.

Once you’re in a class, you stay there; the class doesn’t change over time.

LCA therefore explains long-term **differences between people**: who tends to follow which general pattern.

**2. Hidden Markov Models: Dynamic stages**

In HMMs, the latent variable is a **hidden state that can change over time**.

Instead of assigning each person to one fixed class, HMMs assume that everyone moves through several invisible *phases* during life. For example, “robust,” “beginning to slow down,” or “chronic limitation.”

At each time point, the model estimates the probability of switching from one hidden state to another (the **transition probability**) and the probability of observing a certain health condition (the **emission probability**) while in that state.

HMMs therefore focus on **transitions within people**: how health stages evolve year by year.

In short:

| Feature          | Latent Class Analysis (LCA)                    | Hidden Markov Model (HMM)                          |
| ---------------- | ---------------------------------------------- | -------------------------------------------------- |
| Latent meaning   | Unobserved *group* of individuals              | Unobserved *state* that changes over time          |
| Time dynamics    | Fixed for each person                          | Can change at every time point                     |
| Focus            | Differences *between* people                   | Changes *within* people                            |
| Typical question | “What kinds of overall health patterns exist?” | “How do people move between hidden health stages?” |

To sum up, **LCA groups people**, while **HMMs track phases**:
* LCA sees the life course as belonging to one of a few hidden types;
* HMMs see it as a journey through hidden stages that unfold over time.

## 3. Other important things

### How do we decide the number of types or states?

When we use these methods to describe life trajectories, we always face a practical question:
**How many types (or states) should we identify?**

Overall, the number of groups is largely a **researcher’s choice**, guided by both data and theory. For these three methods, we highly recommend you to use both statistical indicators, which differ across methods, and substantive interpretations with thoery. 

**1. Dissimilarity-based clustering in sequence analysis: Deciding by clustering and interpretation**

After computing pairwise distances between sequences, SA usually applies **hierarchical clustering** (often Ward’s method) to group similar trajectories.

However, the algorithm itself does *not* tell you how many clusters to keep. Researchers have to decide this number by combining:

* **Statistical indicators**, e.g., Average Silhouette Width, Hubert’s C index, or Point Biserial Correlation, and

* **Substantive interpretation**, e.g., whether the clusters make sense theoretically.

> Note: 
> We don't recommend to have too many clusters (e.g., more than 10) as it might be difficult to interprete. 

**2. Latent Class Analysis (LCA): Choosing statistically optimal classes**

In LCA, the number of latent classes is selected by comparing how well models with different class numbers fit the data.

Each model estimates its own likelihood, and we use **information criteria** such as the *Bayesian Information Criterion (BIC)* or *Akaike Information Criterion (AIC)* to decide which one balances fit and simplicity.

Typically, the model with the **lowest BIC** is preferred, as it captures the structure of the data without overfitting. Moreover, substantive interpretation is also very important as you need to assess whether the resulting classes are meaningful, theoretically plausible, and substantively distinct.

**3. Hidden Markov Models (HMMs): estimating the number of hidden states**

HMMs require a similar decision about how many hidden states to include.

Because the model allows transitions over time, too few states may oversimplify real dynamics, while too many may create artificial complexity.

Researchers again rely on **information criteria** such as BIC, AIC, or cross-validation, combined with whether the resulting states are **interpretable** as meaningful life stages.

In practice, one often compares several models (e.g., 4–8 states) and chooses the smallest one that still provides clear, distinct phases.

### Takeaways in one table

Here’s an updated and more precise version of your table, now including a column for **Hidden Markov Models (HMMs)** based on Barban & Billari (2012) and Helske et al. (2018):

### Takeaways in one table

| Aspect              | Dissimilarity-based clustering in SA                                                 | Latent Class Analysis (LCA)                                              | Hidden Markov Models (HMMs)                                                               |
| ------------------- | ---------------------------------------------------------------------- | ------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------- |
| **Philosophy**      | Data-driven: focuses on *how similar* life stories appear              | Model-based: assumes a few hidden group types generate observed sequences | Dynamic model: assumes a hidden process *evolves over time* and generates observed states |
| **Core idea**       | “How far apart are these sequences?”                                   | “Which latent type is each person most likely from?”                     | “What unobserved life stages generate these observed transitions over time?”              |
| **Output**          | Clusters based on dissimilarity                                        | Probabilistic latent classes                                             | Hidden states (life stages) and transition probabilities among them                       |
| **Main tool**       | Distance matrix + cluster analysis (e.g. Ward)                         | Likelihood model + estimated class membership probabilities              | Markov chain with emission and transition matrices estimated via EM algorithm             |
| **Temporal logic**  | Considers sequence order but not an explicit time process              | Ignores time dependence between waves                                    | Explicitly models transitions and temporal dependence between latent states               |
| **Model selection** | Number of clusters chosen by researcher (fit indices + interpretation) | Number of classes chosen by BIC/AIC + interpretability                   | Number of hidden states chosen by BIC/AIC + interpretability of life stages               |
| **Typical use**     | To describe and visualize typical life trajectories                    | To uncover unobserved population types shaping observed patterns         | To model dynamic latent stages and transitions within or across trajectories              |

*Note:*

In the context of HMMs, **“hidden process”** and **“hidden states”** refer to related but slightly different levels of description:

* The **hidden process** is 
  * the *entire underlying stochastic mechanism* that evolves over time according to Markovian rules.
  * In other words, in an HMM, the hidden process is like an underlying storyline that unfolds over time. For example, people move through unobserved life stages such as “career building,” “family formation,” or “retirement preparation.”

* The **hidden states** are the *discrete realizations* (values) that this hidden process takes at each time point.
  * In other words, the hidden states are the specific stages or conditions that the hidden process occupies at each time point. 
  * They are called “discrete realizations” because the model assumes a limited number of possible states (e.g., 4, 6, or 8 stages), and at every time point, the process is in one of them.

Here’s a revised and expanded version of your section **“4. What each method is good (and bad) at”**, where I’ve addressed all your TODOs and made the tone consistent, concise, and balanced — suitable for teaching or documentation.

## 4. What each method is good (and bad) at

Ultimately, the right method depends on your **research question and theoretical focus**.
If your goal is to understand *when* and *in what order* events unfold, sequence analysis (SA) or hidden Markov models (HMMs) are more appropriate.
If your goal is to identify *underlying population types* and relate them to covariates, latent class analysis (LCA) may be a better fit.

Below are the key trade-offs.

### Dissimilarity-based clustering in Sequence Analysis

* **Strengths**

  * Intuitive and transparent: clusters are based on actual similarity in timing and order of events.
  * Easy to visualize using sequence index plots or medoid plots.
  * Flexible choice of distance metrics (Optimal Matching, Hamming, LCS, etc.) lets you emphasize timing, duration, or sequencing.

* **Limitations**

  * Requires choosing and justifying “costs” (e.g., how much one substitution counts), which can influence results.
  * Clustering is *descriptive*—there is no likelihood, no built-in model fit index, and statistical inference (e.g., p-values) is limited.
  * Including covariates or testing effects requires *post hoc* modeling (e.g., multinomial regression on cluster membership), rather than being integrated into the model.

### Latent Class Analysis (LCA)

* **Strengths**

  * Model-based: provides estimated probabilities, fit indices (AIC, BIC, entropy), and uncertainty of classification.
  * Can directly include covariates (e.g., education, cohort, gender) to explain class membership within the same model.
  * Produces “soft” class memberships, acknowledging uncertainty rather than forcing hard boundaries.

* **Limitations**

  * Assumes **local independence**—once you know a person’s latent class, states at different ages are treated as independent.
    This assumption simplifies the model but is often unrealistic for life courses, where what happens at age 20 clearly affects age 21.
  * Focuses on *state distributions at each time point*, not on the *order* of transitions.
    Therefore, two sequences with the same set of states in different orders could be treated as equivalent.

### Hidden Markov Models (HMMs)

* **Strengths**

  * A dynamic extension of LCA that explicitly models **transitions over time**.
  * Each latent state represents an unobserved “phase” of life (e.g., stable employment, instability, withdrawal), and the model estimates **transition probabilities** between them.
  * Captures both *hidden structure* and *temporal dependence* without collapsing time or ignoring order.
  * Can incorporate covariates and provide model-based fit measures.

* **Limitations**

  * Computationally more demanding and sensitive to model specification (number of states, starting values, etc.).
  * Interpretation can be less intuitive, since latent states are not fixed “types” but dynamic regimes people move between.

In summary:

| Focus                                             | Best method           | Why                                               |
| ------------------------------------------------- | --------------------- | ------------------------------------------------- |
| Describing and visualizing real-life trajectories | **Sequence Analysis** | Directly captures observed timing and order       |
| Modeling unobserved population types              | **LCA**               | Groups individuals by shared probability profiles |
| Modeling unobserved dynamic regimes               | **HMMs**              | Captures latent states *and* their transitions    |

## 5. When both are applied to the same data

Studies like **Barban & Billari (2012)** and **Han, Liefbroer & Elzinga (2017)** compared SA and LCA using the same life course data.
Their findings were surprisingly consistent in spirit but different in detail:

* Both found similar *big groups* — e.g., “traditional marriage,” “late marriage,” “singlehood.”
* But SA often merges finer distinctions (e.g., all cohabiters together), while LCA can split them (cohabiters with vs. without children).
* The statistical agreement between the two is moderate — about **0.6 to 0.7** on the adjusted Rand index.

When they checked which classification better predicted real-world factors (like education, religion, or country), **LCA performed slightly better** — probably because it models probabilities directly and can include predictors.

## 6. Choosing between SA and LCA in practice

You can think of the choice this way:

* Use **Sequence Analysis** if you want to *see and describe* how life stories unfold — when events happen, how paths diverge or converge, how complex trajectories look.
* Use **Latent Class Analysis** if you want to *model and explain* differences — estimate class sizes, link them to background factors, or test hypotheses statistically.
* Use **both** if you want a full picture: SA gives the *shape*, LCA gives the *structure*.

## 7. A simple metaphor to remember

Think of 100 songs on your playlist.

* **Sequence Analysis** is like sorting songs by how they *sound*: rhythm, melody, tempo. You cluster them by ear.
* **Latent Class Analysis** is like assuming hidden *genres* (rock, jazz, pop) and estimating which song most likely belongs to which genre.

Both are useful — one listens for similarity, the other explains why things belong together.

## 8. In summary

Sequence Analysis and Latent Class Analysis are **complementary, not competing**.
SA focuses on *pattern recognition*, LCA on *pattern explanation*.

Understanding both helps you see life course data from two angles — the *visible shapes* of people’s lives, and the *hidden structures* behind them.
Together, they give a more complete and meaningful story of how lives unfold over time.

## References

Han, Y., Liefbroer, A. C., & Elzinga, C. H. (2017). Comparing methods of classifying life courses: sequence analysis and latent class analysis. Longitudinal and Life Course Studies, 8(4), 319-341.

Barban, N., & Billari, F. C. (2012). Classifying life course trajectories: a comparison of latent class and sequence analysis. Journal of the Royal Statistical Society Series C: Applied Statistics, 61(5), 765-784.

Hennig, C., & Liao, T. F. (2010). Comparing latent class and dissimilarity based clustering for mixed type variables with application to social stratification. Technical report.

Helske, S., Helske, J., & Eerola, M. (2018). Combining sequence analysis and hidden Markov models in the analysis of complex life sequence data. In Sequence analysis and related approaches: Innovative methods and applications (pp. 185-200). Cham: Springer International Publishing.

---
*We thank [Heyi Zhang](https://profiles.ucl.ac.uk/100967-heyi-zhang) for suggesting us to explain the differences between different methods in life course studies.*

*Author: Yuqi Liang*

